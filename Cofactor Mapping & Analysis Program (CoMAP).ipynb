{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xew4-2zGSJmP"
   },
   "outputs": [],
   "source": [
    "#*** MAIN PROGRAM ***#\n",
    "\n",
    "### USER INTERFACE ###\n",
    "\n",
    "\n",
    "import ipywidgets as widgets  # For interactive UI controls in Jupyter notebooks\n",
    "from IPython.display import display, clear_output  # To display content and handle outputs\n",
    "\n",
    "# User specified values\n",
    "source_organism = \"\"\n",
    "polymer_type = []\n",
    "cofactor_name = []\n",
    "threshold_q_value = 0.0\n",
    "download_pdb_file = False\n",
    "download_unique_list = False\n",
    "download_filtered_overrep = False\n",
    "download_final_cluster_analysis = True\n",
    "\n",
    "# Initalize values for GUI\n",
    "polymer_options = [\"Protein\", \"DNA\", \"RNA\", \"NA-hybrid\", \"Other\"]\n",
    "polymer_entries = []\n",
    "polymer_entries_container = widgets.VBox([])  # Container to dynamically display the appropriate widget\n",
    "\n",
    "cofactor_options = [\"ADENOSINE-5'-TRIPHOSPHATE\", \"ADENOSINE-5'-DIPHOSPHATE\", \"ADENOSINE-5'-MONOPHOSPHATE\", \"ALUMINUM FLUORIDE\",\n",
    "             \"BARIUM ION\", \"BERYLLIUM\", \"BERYLLIUM TRIFLUORIDE ION\", \"BISMUTH(III) ION\", \"CADMIUM ION\", \"CALCIUM ION\", \"CESIUM ION\", \"CHROMIUM ION\", \"COBALT (II) ION\",\n",
    "             \"COBALT (III) ION\", \"COPPER (I) ION\", \"COPPER (II) ION\", \"FE (II) ION\", \"FE (III) ION\", \"FLAVIN-ADENINE DINUCLEOTIDE\", \"GALLIUM (III) ION\", \"GOLD ION\", \"GUANOSINE-5'-TRIPHOSPHATE\",\n",
    "             \"GUANOSINE-5'-DIPHOSPHATE\", \"GUANOSINE-5'-MONOPHOSPHATE\", \"INDIUM (III) ION\", \"IRIDIUM ION\", \"IRIDIUM (III) ION\", \"IRIDIUM HEXAMMINE ION\", \"IRON/SULFUR CLUSTER\", \"LEAD (II) ION\", \"LITHIUM ION\",\n",
    "             \"MAGNESIUM ION\", \"MANGANESE (II) ION\", \"MANGANESE (III) ION\", \"MERCURY (II) IODIDE\", \"MERCURY (II) ION\", \"MOLYBDATE ION\", \"NICKEL (II) ION\", \"NICOTINAMIDE-ADENINE-DINUCLEOTIDE\",\n",
    "             \"OSMIUM ION\", \"OSMIUM 4+ ION\", \"OSMIUM (III) HEXAMMINE\", \"PALLADIUM ION\", \"PLATINUM (II) ION\", \"POTASSIUM ION\", \"RHENIUM\", \"RUBIDIUM ION\", \"RUTHENIUM ION\", \"RUTHENIUM OCTASPORINE 4\",\n",
    "             \"RUTHENIUM PYRIDOCARBAZOLE\", \"SILVER ION\", \"SODIUM ION\", \"STRONTIUM ION\", \"THALLIUM (I) ION\", \"TITANIUM ION\", \"TUNGSTATE(VI)ION\", \"TUNGSTEN ION\", \"VANADATE ION\", \"YTTERBIUM (III) ION\",\n",
    "             \"YTTRIUM (III) ION\", \"ZINC ION\", \"ZIRCONIUM ION\"]\n",
    "cofactor_entries = []\n",
    "cofactor_entries_container = widgets.VBox([])  # Container to dynamically display the appropriate widget\n",
    "\n",
    "# Define Dropdown Widgets + Options\n",
    "dropdown_organism = widgets.Dropdown(\n",
    "    options=[\"Homo sapiens (human)\", \"Mus musculus (mouse)\", \"Saccharomyces cerevisiae (yeast)\"],\n",
    "    value=\"Homo sapiens (human)\",  # Default value\n",
    ")\n",
    "\n",
    "add_button_p = widgets.Button(\n",
    "    description=\"Add Polymer Entity\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "remove_button_p = widgets.Button(\n",
    "    description=\"Remove Polymer Entity\",\n",
    "    button_style=\"danger\",\n",
    "    disbaled=True  # Cannot remove the first cofactor, must enter at least one\n",
    ")\n",
    "\n",
    "and_or_button_p = widgets.Button(\n",
    "    description=\"AND/OR Button\",\n",
    "    button_style=\"\",\n",
    "    disabled=True  # Doesn't apply until more than one cofactor\n",
    ")\n",
    "\n",
    "add_button_c = widgets.Button(\n",
    "    description=\"Add Cofactor\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "remove_button_c = widgets.Button(\n",
    "    description=\"Remove Cofactor\",\n",
    "    button_style=\"danger\",\n",
    "    disbaled=True  # Cannot remove the first cofactor, must enter at least one\n",
    ")\n",
    "\n",
    "and_or_button_c = widgets.Button(\n",
    "    description=\"AND/OR Button\",\n",
    "    button_style=\"\",\n",
    "    disabled=True  # Doesn't apply until more than one cofactor\n",
    ")\n",
    "\n",
    "# Define Float Input Widget\n",
    "float_q_value_input = widgets.FloatText(\n",
    "    value=0.05,\n",
    ")\n",
    "\n",
    "# Define True/False Dropdown Widgets\n",
    "TF_PDB_file = widgets.Dropdown(\n",
    "  options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=True,  # Default value\n",
    ")\n",
    "\n",
    "TF_unique_list = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=True,  # Default value\n",
    ")\n",
    "\n",
    "TF_filtered_overrep = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=True,  # Default value\n",
    ")\n",
    "\n",
    "TF_final_cluster_analysis = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=True,  # Default value\n",
    ")\n",
    "\n",
    "# Define Submit Button\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit\",\n",
    "    button_style=\"primary\", # Button style\n",
    ")\n",
    "\n",
    "add_remove_widgets_p = widgets.HBox([add_button_p, remove_button_p, and_or_button_p])\n",
    "add_remove_widgets_c = widgets.HBox([add_button_c, remove_button_c, and_or_button_c])\n",
    "\n",
    "# Create Labels (Ensuring Descriptions are Fully Visible)\n",
    "label1 = widgets.Label(\"Select source organism:\")\n",
    "label2 = widgets.Label(\"Enter threshold q-value:\")\n",
    "label3 = widgets.Label(\"Download PDB entries file:\")\n",
    "label4 = widgets.Label(\"Download unique Ensembl gene list:\")\n",
    "label5 = widgets.Label(\"Download filtered overrepresentation analysis:\")\n",
    "label6 = widgets.Label(\"Download final cluster analysis:\")\n",
    "\n",
    "# Use HBox to Align Labels and Widgets\n",
    "row1 = widgets.HBox([label1, dropdown_organism])\n",
    "row2 = widgets.HBox([label2, float_q_value_input])\n",
    "row3 = widgets.HBox([label3, TF_PDB_file])\n",
    "row4 = widgets.HBox([label4, TF_unique_list])\n",
    "row5 = widgets.HBox([label5, TF_filtered_overrep])\n",
    "row6 = widgets.HBox([label6, TF_final_cluster_analysis])\n",
    "\n",
    "# Output Display\n",
    "output = widgets.Output()\n",
    "\n",
    "# Enable remove + and/or button if more than one cofactor\n",
    "def update_button_state():\n",
    "  remove_button_c.disabled = len(cofactor_entries) <= 1\n",
    "  and_or_button_c.disabled = len(cofactor_entries) <= 1\n",
    "\n",
    "  add_button_p.disabled = len(polymer_entries) >= 5\n",
    "  remove_button_p.disabled = len(polymer_entries) <= 1\n",
    "  and_or_button_p.disabled = len(polymer_entries) <= 1\n",
    "\n",
    "# Function to create a polymer entry\n",
    "def create_polymer_entry(index, removable=True):\n",
    "  # Create dropdown widget\n",
    "  dropdown_polymer = widgets.Dropdown(\n",
    "      options=polymer_options,\n",
    "      value=\"Protein\",\n",
    "      description=f\"Polymer {index}:\",\n",
    "      layout=widgets.Layout(width='390px'),\n",
    "  )\n",
    "\n",
    "  # Container that holds all parts of this entry\n",
    "  entry_box = widgets.HBox([dropdown_polymer])\n",
    "\n",
    "  polymer_entries.append((entry_box, dropdown_polymer, removable))\n",
    "  polymer_entries_container.children += (entry_box,)  # Adds the entry to the visible GUI\n",
    "\n",
    "  update_button_state()\n",
    "\n",
    "# Function to create a cofactor entry\n",
    "def create_cofactor_entry(index, removable=True):\n",
    "  # Create the two toggle buttons for selection\n",
    "  select_co_buttons = widgets.ToggleButtons(\n",
    "      options=['Dropdown', 'Custom'],\n",
    "      description=f\"Cofactor {index}:\",\n",
    "      button_style='info',\n",
    "      value=None\n",
    "  )\n",
    "\n",
    "  # Create dropdown widget\n",
    "  dropdown_cofactor = widgets.Dropdown(\n",
    "      options=cofactor_options,\n",
    "      value=\"ADENOSINE-5'-TRIPHOSPHATE\",\n",
    "      description=\"Select:\",\n",
    "      layout=widgets.Layout(width='400px'),\n",
    "      visible=False  # Hidden by default\n",
    "  )\n",
    "\n",
    "  # Create the text box widget\n",
    "  custom_cofactor_widget = widgets.Text(\n",
    "      placeholder='Type cofactor name exactly as it appears in PDB...',\n",
    "      description=\"Enter:\",\n",
    "      layout=widgets.Layout(width='400px'),\n",
    "      visible=False  # Hidden by default\n",
    "  )\n",
    "\n",
    "  # Container that holds all parts of this entry\n",
    "  entry_box = widgets.HBox([select_co_buttons])\n",
    "\n",
    "  # Function to show either the dropdown or textbox based on button selection\n",
    "  def toggle_input(change):\n",
    "      widgets_to_show = [select_co_buttons]\n",
    "      selection = change['new']\n",
    "\n",
    "      if selection == 'Dropdown':\n",
    "          dropdown_cofactor.visible = True\n",
    "          custom_cofactor_widget.visible = False\n",
    "          widgets_to_show.append(dropdown_cofactor)\n",
    "      elif selection == 'Custom':\n",
    "          dropdown_cofactor.visible = False\n",
    "          custom_cofactor_widget.visible = True\n",
    "          widgets_to_show.append(custom_cofactor_widget)\n",
    "      else:\n",
    "          dropdown_cofactor.visible = False\n",
    "          custom_cofactor_widget.visible = False\n",
    "\n",
    "      entry_box.children = widgets_to_show\n",
    "\n",
    "  # Attach event listener to the selection buttons\n",
    "  select_co_buttons.observe(toggle_input, names='value')\n",
    "\n",
    "  cofactor_entries.append((entry_box, select_co_buttons, dropdown_cofactor, custom_cofactor_widget, removable))\n",
    "  cofactor_entries_container.children += (entry_box,)  # Adds the entry to the visible GUI\n",
    "\n",
    "  update_button_state()\n",
    "\n",
    "# Function to add polymer entry\n",
    "def add_polymer_entry(b):\n",
    "  create_polymer_entry(len(polymer_entries) + 1)\n",
    "  update_button_state()\n",
    "\n",
    "# Function to add cofactor entry\n",
    "def add_cofactor_entry(b):\n",
    "  create_cofactor_entry(len(cofactor_entries) + 1)\n",
    "\n",
    "# Function to remove polymer entry, ensuring at least one remains\n",
    "def remove_polymer_entry(b):\n",
    "  if len(polymer_entries) > 1:\n",
    "    last_entry, _, _ = polymer_entries.pop()\n",
    "    polymer_entries_container.children = polymer_entries_container.children[:-1]\n",
    "\n",
    "  update_button_state()\n",
    "\n",
    "# Function to remove cofactor entry, ensuring at least one remains\n",
    "def remove_cofactor_entry(b):\n",
    "  if len(cofactor_entries) > 1:\n",
    "    last_entry, _, _, _, _ = cofactor_entries.pop()\n",
    "    cofactor_entries_container.children = cofactor_entries_container.children[:-1]\n",
    "\n",
    "  update_button_state()\n",
    "\n",
    "# Function to update and/or operator\n",
    "def and_or_click(b):\n",
    "  if b.description == \"AND\":\n",
    "    b.description = \"OR\"\n",
    "  else:\n",
    "    b.description = \"AND\"\n",
    "\n",
    "# Function to run program after submit button clicked\n",
    "def on_button_click(b):  # Run code after user inputs submitted\n",
    "    global source_organism, polymer_type, cofactor_name, threshold_q_value, download_pdb_file, download_unique_list, download_filtered_overrep, download_final_cluster_analysis, filtered_P_and_Q_file\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "\n",
    "        # Assign source organism dropdown value to variable\n",
    "        if \"Homo sapiens (human)\" in dropdown_organism.value:\n",
    "          source_organism = \"Homo sapiens\"\n",
    "        elif \"Mus musculus (mouse)\" in dropdown_organism.value:\n",
    "          source_organism = \"Mus musculus\"\n",
    "        elif \"Saccharomyces cerevisiae (yeast)\" in dropdown_organism.value:\n",
    "          source_organism = \"Saccharomyces cerevisiae\"\n",
    "\n",
    "        # Assign input values to cofactor name\n",
    "        for idx, (entry_box, toggle, dropdown, custom, _) in enumerate(cofactor_entries, start=1):\n",
    "            if toggle.value is None:\n",
    "                print(f\"Please select Dropdown or Custom for Cofactor {idx}.\")\n",
    "                return\n",
    "\n",
    "            if toggle.value == \"Dropdown\":\n",
    "                cofactor = dropdown.value\n",
    "            elif toggle.value == \"Custom\":\n",
    "                cofactor = custom.value.strip()\n",
    "\n",
    "            if not cofactor:\n",
    "                print(f\"Please enter or select a cofactor name for Cofactor {idx}.\")\n",
    "                return\n",
    "\n",
    "            if len(cofactor_entries) <= 1:\n",
    "              operator_c = None\n",
    "            elif and_or_button_c.description == \"AND/OR Button\" and len(cofactor_entries) > 1:\n",
    "              operator_c = None\n",
    "              print(\"Please select an operator for your cofactor names by pressing the AND/OR Button.\")\n",
    "              return\n",
    "            else:\n",
    "              operator_c = and_or_button_c.description\n",
    "\n",
    "            cofactor_name.append(cofactor)\n",
    "\n",
    "        # Assign input values to polymer entity\n",
    "        for idx, (entry_box, dropdown, _) in enumerate(polymer_entries, start=1):\n",
    "            polymer = dropdown.value\n",
    "\n",
    "            if len(polymer_entries) <= 1:\n",
    "              operator_p = None\n",
    "            elif and_or_button_p.description == \"AND/OR Button\" and len(polymer_entries) > 1:\n",
    "              operator_p = None\n",
    "              print(\"Please select an operator for your polymer entities by pressing the AND/OR Button.\")\n",
    "              return\n",
    "            else:\n",
    "              operator_p = and_or_button_p.description\n",
    "\n",
    "            polymer_type.append(polymer)\n",
    "\n",
    "        if float_q_value_input.value <= 0:\n",
    "          threshold_q_value = 0.05\n",
    "        else:\n",
    "          threshold_q_value = float_q_value_input.value\n",
    "\n",
    "        download_pdb_file = TF_PDB_file.value\n",
    "        download_unique_list = TF_unique_list.value\n",
    "        download_filtered_overrep = TF_filtered_overrep.value\n",
    "        download_final_cluster_analysis = TF_final_cluster_analysis.value\n",
    "\n",
    "        # Print retrieved values\n",
    "        print(f\"\\nSource organism: {source_organism}\")\n",
    "        print(f\"Polymer entity type: {polymer_type}\")\n",
    "        print(f\"Polymer operator: {operator_p}\")\n",
    "        print(f\"Cofactor: {cofactor_name}\")\n",
    "        print(f\"Cofactor operator: {operator_c}\")\n",
    "        print(f\"Threshold q-value: {threshold_q_value}\")\n",
    "        print(f\"Download PDB entries file: {download_pdb_file}\")\n",
    "        print(f\"Download unique Ensembl gene list: {download_unique_list}\")\n",
    "        print(f\"Download filtered overrepresentation analysis: {download_filtered_overrep}\")\n",
    "        print(f\"Download final cluster analysis: {download_final_cluster_analysis}\\n\")\n",
    "\n",
    "        # Disable buttons after submission\n",
    "        submit_button.disabled = True\n",
    "\n",
    "        add_button_c.disabled = True\n",
    "        remove_button_c.disabled = True\n",
    "        and_or_button_c.disabled = True\n",
    "\n",
    "        add_button_p.disabled = True\n",
    "        remove_button_p.disabled = True\n",
    "        and_or_button_p.disabled = True\n",
    "\n",
    "    ### PACKAGE INSTALLATIONS ###\n",
    "\n",
    "\n",
    "    import os\n",
    "    import subprocess\n",
    "    import sys\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Function to check if a package is installed\n",
    "    def is_package_installed(package):\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", package], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)  # Supresses standard output and error messages\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            return False\n",
    "\n",
    "    # Function to install a package with a progress bar\n",
    "    def install_package(package):\n",
    "        if not is_package_installed(package):\n",
    "            with tqdm(desc=f\"Installing {package}\", ascii=True, ncols=80, unit=\"B\", unit_scale=True) as progress:  # Sets up the progress bar\n",
    "                process = subprocess.Popen(\n",
    "                    [sys.executable, \"-m\", \"pip\", \"install\", package],  # Runs pip install for the package\n",
    "                    stdout=subprocess.PIPE,  # Captures standard output\n",
    "                    stderr=subprocess.STDOUT,  # Redirects errors to stdout\n",
    "                    text=True,   # Ensures output is treated as a text stream\n",
    "                    bufsize=1  # Get updates instead of waiting for the process to finish\n",
    "                )\n",
    "\n",
    "                for line in process.stdout:\n",
    "                    progress.update(len(line))  # Update the progress bar\n",
    "\n",
    "                process.wait()  # Blocks execution until package installed\n",
    "                progress.close()  # Closes the progress bar\n",
    "\n",
    "    # Install required packages\n",
    "    install_package(\"rcsb-api\")\n",
    "    install_package(\"selenium\")\n",
    "\n",
    "    # Function to check if a command is available\n",
    "    def is_command_available(command):\n",
    "        return subprocess.call([\"which\", command], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) == 0\n",
    "\n",
    "    # Function to run a command with a progress bar\n",
    "    def run_command_with_progress(command, desc):\n",
    "        with tqdm(desc=desc, ascii=True, ncols=80, unit=\"B\", unit_scale=True) as progress:\n",
    "            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)  # Runs the command\n",
    "\n",
    "            for line in process.stdout:\n",
    "                progress.update(len(line))  # Update the progress bar\n",
    "\n",
    "            process.wait()  # Blocks execution until the command is complete\n",
    "            progress.close()  # Closes the progress bar\n",
    "\n",
    "    # Install Microsoft Edge browser if not installed\n",
    "    edge_version = \"133.0.3065.82-1\"\n",
    "    edge_deb = f\"microsoft-edge-stable_{edge_version}_amd64.deb\"\n",
    "\n",
    "    if not is_command_available(\"microsoft-edge\"):\n",
    "        run_command_with_progress([\"apt-get\", \"update\"], \"Updating package lists\")\n",
    "        run_command_with_progress([\"wget\", f\"https://packages.microsoft.com/repos/edge/pool/main/m/microsoft-edge-stable/{edge_deb}\"], \"Downloading Microsoft Edge\")\n",
    "        run_command_with_progress([\"dpkg\", \"-i\", edge_deb], \"Installing Microsoft Edge\")\n",
    "        run_command_with_progress([\"apt-get\", \"-f\", \"install\", \"-y\"], \"Resolving dependencies\")\n",
    "\n",
    "    # Install Microsoft Edge WebDriver if not installed\n",
    "    edge_driver_version = \"133.0.3065.82\"\n",
    "    edge_driver_path = \"/usr/bin/edgedriver\"\n",
    "\n",
    "    if not os.path.exists(edge_driver_path):\n",
    "        run_command_with_progress([\"wget\", f\"https://msedgedriver.azureedge.net/{edge_driver_version}/edgedriver_linux64.zip\"], \"Downloading Edge WebDriver\")\n",
    "        run_command_with_progress([\"unzip\", \"edgedriver_linux64.zip\"], \"Unzipping WebDriver\")\n",
    "        run_command_with_progress([\"mv\", \"msedgedriver\", edge_driver_path], \"Installing Edge WebDriver\")\n",
    "\n",
    "\n",
    "    # Import necessary modules\n",
    "    from rcsbapi.search import search_attributes as attrs  # To search the PDB\n",
    "    import requests  # To send requests to interact with web APIs\n",
    "    import json  # To parse and format JSON data\n",
    "    import pandas as pd  # To use data structures for data analysis and manipulation\n",
    "    import csv  # To handle csv files\n",
    "    from google.colab import files  # For file upload and download in Colab\n",
    "    # To automate web page interactions\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.edge.options import Options\n",
    "    from selenium.webdriver.edge.service import Service as EdgeService\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import Select\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    import io  # To handle in-memory text\n",
    "    import math  # To access mathematical functions\n",
    "    from scipy.stats import hypergeom  # To calculate p-values for overlap significance\n",
    "    from statsmodels.stats.multitest import multipletests  # For correcting multiple hypothesis testing (Bonferroni correction)\n",
    "    import numpy as np  # For numerical operations and matrix manipulations\n",
    "    from scipy.cluster.hierarchy import linkage, dendrogram, fcluster  # For hierarchical clustering and dendrogram visualization\n",
    "    from scipy.spatial.distance import squareform  # To convert a condensed distance matrix into a square form\n",
    "    import matplotlib.pyplot as plt  # For creating plots\n",
    "    from itertools import combinations  # Generates all possible unique pairs of elements\n",
    "    from collections import Counter  # To count the occurrences of elements in an iterable\n",
    "    import re  # To extract words from text\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    ### DATA ANALYSIS ###\n",
    "\n",
    "\n",
    "    ## STEP 1: Curation of Protein List using PDB ##\n",
    "    print(\"\\nRetriving entries from the PDB...\")\n",
    "\n",
    "    # Construct an RCSB query to search for structures\n",
    "    query1 = attrs.rcsb_entity_source_organism.scientific_name == source_organism\n",
    "    query2 = []\n",
    "    query3 = []\n",
    "\n",
    "    for polymer in polymer_type:\n",
    "      query2.append(attrs.entity_poly.rcsb_entity_polymer_type == polymer)\n",
    "\n",
    "    for cofactor in cofactor_name:\n",
    "      query3.append(attrs.chem_comp.name == cofactor)\n",
    "\n",
    "    # Combine all polymers into a single grouped expression\n",
    "    if query2:  # Make sure query2 isn't empty\n",
    "        polymer_query = query2[0]\n",
    "        for polymer in query2[1:]:\n",
    "            if operator_p == \"AND\":\n",
    "                polymer_query = polymer_query & polymer\n",
    "            else:\n",
    "                polymer_query = polymer_query | polymer\n",
    "\n",
    "    # Combine all cofactors into a single grouped expression\n",
    "    if query3:  # Make sure query3 isn't empty\n",
    "        cofactor_query = query3[0]\n",
    "        for cofactor in query3[1:]:\n",
    "            if operator_c == \"AND\":\n",
    "                cofactor_query = cofactor_query & cofactor\n",
    "            else:\n",
    "                cofactor_query = cofactor_query | cofactor\n",
    "\n",
    "        # Now add the whole cofactor group to the main query\n",
    "        query = query1 & (polymer_query) & (cofactor_query)\n",
    "\n",
    "    # Execute query and construct a list of the results\n",
    "    results = list(query())\n",
    "\n",
    "    # Replace potential characters in cofactor names that cannot be used in file names\n",
    "    for i in range(len(cofactor_name)):\n",
    "        cofactor = cofactor_name[i]\n",
    "        cofactor = cofactor.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"*\", \"_\").replace(\":\", \"_\")\n",
    "        cofactor_name[i] = cofactor\n",
    "\n",
    "    # The maximum number of structures to be processed at one time\n",
    "    batch_size = 5000\n",
    "    # Store file names\n",
    "    generated_files = []\n",
    "\n",
    "    step1_seconds = (math.ceil(len(results)/batch_size))*12\n",
    "    print(f\"\\nStep 1 (~{step1_seconds} sec): \\nRetrieved {len(results)} entries from the Protein Data Bank\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "      print(\"\\nERROR: The program has exited as this combination of queries returns 0 results. Try selecting different values\")\n",
    "      return\n",
    "\n",
    "    # Function to save the entry ID, PDB ID, macromolecule name, gene name, and accession code(s) of each returned entry\n",
    "    def fetch_and_save_batch(batch, batch_index):\n",
    "        formatted_string = \", \".join(f'\"{entry}\"' for entry in batch)  # Properly formatting the entries in each batch\n",
    "\n",
    "        # GraphQL query provided by RCSB\n",
    "        graphql_query = f\"\"\"\n",
    "        {{\n",
    "          entries(entry_ids: [{formatted_string}]) {{\n",
    "            rcsb_id\n",
    "            rcsb_entry_container_identifiers {{\n",
    "              entry_id\n",
    "            }}\n",
    "            polymer_entities {{\n",
    "              rcsb_entity_source_organism {{\n",
    "                rcsb_gene_name {{\n",
    "                  value\n",
    "                }}\n",
    "              }}\n",
    "              rcsb_polymer_entity {{\n",
    "                rcsb_macromolecular_names_combined {{\n",
    "                  name\n",
    "                }}\n",
    "              }}\n",
    "              rcsb_polymer_entity_container_identifiers {{\n",
    "                reference_sequence_identifiers {{\n",
    "                  database_accession\n",
    "                }}\n",
    "              }}\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # GraphQL endpoint\n",
    "        url_RCSB = \"https://data.rcsb.org/graphql\"\n",
    "\n",
    "        # Send the request with the GraphQL query\n",
    "        response = requests.post(\n",
    "            url_RCSB,\n",
    "            json={\"query\": graphql_query},\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "\n",
    "        # Check if the request was successful (status code = 200)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Extract relevant data from the response\n",
    "            report_data = []\n",
    "            for entry in data['data']['entries']:\n",
    "                pdb_id = entry.get('rcsb_id', \"\")  # Store the entries PDB ID\n",
    "                entry_ids = entry.get('rcsb_entry_container_identifiers', {}).get('entry_id', \"\")  # Store the entries entry ID\n",
    "\n",
    "                if 'polymer_entities' in entry and entry['polymer_entities']:\n",
    "                    for polymer_entity in entry['polymer_entities']:\n",
    "                        gene_names = []\n",
    "                        macromolecules = []\n",
    "                        codes = []\n",
    "\n",
    "                        # Extract gene names as a 2D list, one sublist for each organism\n",
    "                        if 'rcsb_entity_source_organism' in polymer_entity and polymer_entity['rcsb_entity_source_organism']:\n",
    "                            gene_names = [[gene['value'] for gene in organism['rcsb_gene_name']]\n",
    "                                          if 'rcsb_gene_name' in organism and organism['rcsb_gene_name'] else [\"\"]\n",
    "                                          for organism in polymer_entity['rcsb_entity_source_organism']]\n",
    "                        if not gene_names:\n",
    "                            gene_names = [[\"\"]]  # Store a blank entry\n",
    "\n",
    "                        # Extract macromolecule names for entry\n",
    "                        if 'rcsb_polymer_entity' in polymer_entity and polymer_entity['rcsb_polymer_entity']:\n",
    "                            organism = polymer_entity['rcsb_polymer_entity']\n",
    "                            if 'rcsb_macromolecular_names_combined' in organism and organism['rcsb_macromolecular_names_combined']:\n",
    "                                macromolecules = [gene['name'] for gene in organism['rcsb_macromolecular_names_combined']]\n",
    "                        if not macromolecules:\n",
    "                            macromolecules = [\"\"]  # Store a blank entry\n",
    "\n",
    "                        # Extract accession codes for entry\n",
    "                        if 'rcsb_polymer_entity_container_identifiers' in polymer_entity and polymer_entity['rcsb_polymer_entity_container_identifiers']:\n",
    "                            organism = polymer_entity['rcsb_polymer_entity_container_identifiers']\n",
    "                            if 'reference_sequence_identifiers' in organism and organism['reference_sequence_identifiers']:\n",
    "                                codes = [gene['database_accession'] for gene in organism['reference_sequence_identifiers']]\n",
    "                        if not codes:\n",
    "                            codes = [\"\"]  # Store a blank entry\n",
    "\n",
    "                        # Generate rows for each organism's gene set\n",
    "                        for gene_set in gene_names:\n",
    "                            report_data.append({\n",
    "                                'Entry ID': entry_ids,\n",
    "                                'PDB ID': pdb_id,\n",
    "                                'Gene Name': ', '.join(gene_set),\n",
    "                                'Macromolecule Name': ', '.join(macromolecules),\n",
    "                                'Accession Code(s)': ', '.join(codes)\n",
    "                            })\n",
    "\n",
    "            # Convert the extracted data into a DataFrame and save as CSV\n",
    "            df = pd.DataFrame(report_data)\n",
    "            filename = f\"custom_report_batch_{batch_index}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            generated_files.append(filename)\n",
    "            print(f\"Custom report saved as '{filename}'\")\n",
    "        # Timeout error making the request\n",
    "        elif response.status_code == 540:\n",
    "          print(f\"Timeout error encountered when accessing {url_RCSB}. Please try running the program again.\")\n",
    "          print(response.status_code, response.text)\n",
    "          sys.exit()  # Force program to exit\n",
    "        # Error making the request\n",
    "        else:\n",
    "            print(f\"Error accessing {url_RCSB}:\", response.status_code, response.text)\n",
    "            sys.exit()  # Force program to exit\n",
    "\n",
    "    # Process entries in batches so as not to lose any data\n",
    "    for i in range(0, len(results), batch_size):\n",
    "        batch = results[i:i+batch_size]\n",
    "        fetch_and_save_batch(batch, i // batch_size + 1)  # Function call\n",
    "\n",
    "    # Function to combine all saved CSV files containing structure information into one\n",
    "    def combine_csv_files(output_path):\n",
    "      # If multiple files were generated, combine them\n",
    "      if len(generated_files) > 1:\n",
    "        combined_rows = []\n",
    "        headers_written = False\n",
    "\n",
    "        for file_name in generated_files:\n",
    "          with open(file_name, 'r', newline='') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            headers = next(csv_reader)\n",
    "\n",
    "            # Write the header once\n",
    "            if not headers_written:\n",
    "                    combined_rows.append(headers)\n",
    "                    headers_written = True\n",
    "\n",
    "            for row in csv_reader:\n",
    "              combined_rows.append(row)\n",
    "\n",
    "        # Write the combined data to a new CSV file\n",
    "        with open(output_path, 'w', newline='') as output_csv:\n",
    "          csv_writer = csv.writer(output_csv)\n",
    "          csv_writer.writerows(combined_rows)\n",
    "\n",
    "        # Download file if requested\n",
    "        if download_pdb_file == True:\n",
    "          files.download(output_path)\n",
    "\n",
    "        print(f\"Combined custom reports saved as '{output_path}'\")\n",
    "        return output_path\n",
    "\n",
    "      # If only one file was generated, download it\n",
    "      else:\n",
    "        if download_pdb_file == True:\n",
    "          files.download(generated_files[0])\n",
    "        return generated_files[0]\n",
    "\n",
    "    # Function to extract the unique accession codes from the combined file\n",
    "    def accession_code_set(file_path):\n",
    "      code_list = set()\n",
    "\n",
    "      with open(file_path, \"r\", newline='') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        headers = next(csv_reader)\n",
    "\n",
    "        for row in csv_reader:\n",
    "          if len(row) >= 5:\n",
    "            entries = row[4].split(',')\n",
    "\n",
    "            for entry in entries:\n",
    "              if entry and entry.strip():\n",
    "                code_list.add(entry.strip())\n",
    "\n",
    "      print(f\"{len(code_list)} unique accession codes found\")\n",
    "      return code_list\n",
    "\n",
    "    # File path\n",
    "    combined_report = \"Combined_custom_report.csv\"\n",
    "\n",
    "    # Function calls\n",
    "    combined_pdb_file = combine_csv_files(combined_report)\n",
    "    accessionCodes_set = accession_code_set(combined_pdb_file)\n",
    "\n",
    "\n",
    "    ## STEP 2: Convert Accession Codes to Ensembl IDs and Save ##\n",
    "\n",
    "    # Maximum number of accession codes to process at one time\n",
    "    batch_size_codes = 500\n",
    "    # Store file names\n",
    "    generated_codes = []\n",
    "\n",
    "    # Convert set of accession codes to a list\n",
    "    accessionCodes_list = [item for item in accessionCodes_set]\n",
    "\n",
    "    step2_seconds = (math.ceil(len(accessionCodes_set)/batch_size_codes))*25\n",
    "    print(f\"\\nStep 2 (~{step2_seconds} sec):\")\n",
    "\n",
    "    # Function to convert accession codes to their Ensembl IDs using DAVID\n",
    "    def fetch_converted_codes(batch, batch_index):\n",
    "      formatted_codes = \", \".join(entry for entry in batch)  # Properly format the accession codes for the API\n",
    "\n",
    "      '''# Define API endpoint\n",
    "      url_DAVID = f\"https://davidbioinformatics.nih.gov/api.jsp?type=UNIPROT_ACCESSION&ids={formatted_codes}&tool=conversion\"\n",
    "\n",
    "      # Make GET request\n",
    "      response = requests.get(url_DAVID)'''\n",
    "\n",
    "      url_DAVID = f\"https://davidbioinformatics.nih.gov/conversion.jsp\"\n",
    "\n",
    "      # Make GET request\n",
    "      response = requests.get(url_DAVID)\n",
    "\n",
    "      # Check if the request was successful (status code = 200)\n",
    "      if response.status_code == 200:\n",
    "          # Define options for how Selenium will access the website\n",
    "          options = Options()\n",
    "          options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "          options.add_argument(\"--disable-gpu\")  # For Linux compatibility\n",
    "          options.add_argument(\"--no-sandbox\")  # Needed in Colab\n",
    "          options.add_argument(\"--disable-dev-shm-usage\")  # Prevents memory issues\n",
    "\n",
    "          # Specify the executable path and create the Edge Webdriver instance\n",
    "          edge_service = EdgeService(executable_path='/usr/bin/edgedriver')\n",
    "          driver = webdriver.Edge(service=edge_service, options=options)\n",
    "          driver.get(url_DAVID)\n",
    "\n",
    "          # Upload the accession code list into the correct box\n",
    "          list_box = driver.find_element(By.ID, \"LISTBox\")\n",
    "          list_box.send_keys(formatted_codes)\n",
    "\n",
    "          # Select Uniprot Accession as its current ID type\n",
    "          id_dropdown = driver.find_element(By.NAME, \"Identifier\")\n",
    "          select = Select(id_dropdown)\n",
    "          select.select_by_value(\"UNIPROT_ACCESSION\")\n",
    "\n",
    "          # Select gene list as the type of upload list\n",
    "          gene_button = driver.find_element(By.NAME, \"rbUploadType\")\n",
    "          gene_button.click()\n",
    "\n",
    "          # Submit the list\n",
    "          submit_list = driver.find_element(By.NAME, \"B52\")\n",
    "          submit_list.click()\n",
    "\n",
    "          # Handle pop-up alert if present\n",
    "          try:\n",
    "              WebDriverWait(driver, 3).until(EC.alert_is_present())  # Wait for alert\n",
    "              alert = driver.switch_to.alert\n",
    "              alert.accept()\n",
    "          except:\n",
    "              pass\n",
    "\n",
    "          # Navigate back to the gene ID conversion tool\n",
    "          tool_dropdown = driver.find_element(By.XPATH, '//a[@id=\"navbarDropdown\" and normalize-space(text())=\"Shortcut to DAVID Tools\"]')\n",
    "          tool_dropdown.click()\n",
    "          conversion_tool = driver.find_element(By.XPATH, '//a[@href=\"conversion.jsp\"]')\n",
    "          conversion_tool.click()\n",
    "\n",
    "          # Select Ensembl ID as the type to be converted to\n",
    "          type_box = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.NAME, \"convertTo\")))\n",
    "          select = Select(type_box)\n",
    "          select.select_by_value(\"ENSEMBL_GENE_ID\")\n",
    "\n",
    "          # Identify the species search box\n",
    "          species_search_box = driver.find_element(By.NAME, \"convertSpeciesSelect\")\n",
    "          species_search_box.send_keys(source_organism)  # Type in the organism name\n",
    "\n",
    "          # Wait for the species name to appear in the dropdown and click it\n",
    "          suggestion = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, f\"//div[contains(@class, 'tt-suggestion') and text()='{source_organism}']\")))\n",
    "          suggestion.click()\n",
    "\n",
    "          # Identify the submit button and click it\n",
    "          submit_conversion = driver.find_element(By.NAME, \"Submit\")\n",
    "          submit_conversion.click()\n",
    "\n",
    "          # Get all open window handles\n",
    "          all_tabs = driver.window_handles\n",
    "          # Switch to the newly opened tab with the converted Ensembl IDs\n",
    "          driver.switch_to.window(all_tabs[-1])\n",
    "\n",
    "          # Identify the download button and click it\n",
    "          try:\n",
    "              download_button = WebDriverWait(driver, 60).until(EC.element_to_be_clickable((By.LINK_TEXT, \"Download File\")))\n",
    "              download_button.click()\n",
    "          except TimeoutException:\n",
    "              print(\"\\nERROR: The program has exited as no download button has been found. It is likely that 0 IDs have been converted. Try selecting different values\")\n",
    "              driver.quit()  # Ensures the browser closes properly if needed\n",
    "              sys.exit()  # Force program to exit\n",
    "\n",
    "          # Get the href attribute of the download file (the URL of the text file)\n",
    "          download_url = download_button.get_attribute('href')\n",
    "\n",
    "          # Download the text file using requests\n",
    "          response = requests.get(download_url)\n",
    "\n",
    "          # Save the content to a .txt file\n",
    "          filename = f\"accession_code_batch_{batch_index}.txt\"\n",
    "          with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "              file.write(response.text)\n",
    "\n",
    "          generated_codes.append(filename)\n",
    "          print(f\"Accession codes successfully converted to Ensembl ID. File saved as '{filename}'\")\n",
    "\n",
    "          # Close the Selenium driver\n",
    "          driver.quit()\n",
    "      # Error accessing the request\n",
    "      else:\n",
    "          print(f\"Error accessing {url_DAVID}:\", response.status_code, response.text)\n",
    "          sys.exit()  # Force program to exit\n",
    "\n",
    "    # Process entries in batches so as not to lose data\n",
    "    for i in range(0, len(accessionCodes_list), batch_size_codes):\n",
    "        batch = accessionCodes_list[i:i+batch_size_codes]\n",
    "        fetch_converted_codes(batch, i // batch_size_codes + 1)  # Function call\n",
    "\n",
    "    # Function to combine all saved TXT files continaing Ensembl IDs into one\n",
    "    def combine_txt_files(output_path):\n",
    "        headers_written = False\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for txt_file_path in generated_codes:\n",
    "                # Open the file and read its contents\n",
    "                with open(txt_file_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                    txt_reader = csv.reader(txt_file, delimiter='\\t')  # Specify tab-separated file\n",
    "\n",
    "                    for row_index, row in enumerate(txt_reader):\n",
    "                      if row_index == 0:\n",
    "                        # Write header once\n",
    "                        if not headers_written:\n",
    "                          outfile.write('\\t'.join(row) + '\\n')\n",
    "                          headers_written = True\n",
    "                      else:\n",
    "                        outfile.write('\\t'.join(row) + '\\n')  # Write the content to one TXT file\n",
    "\n",
    "        print(f\"Ensembl ID txt files successfully combined. File saved as '{output_path}'\")\n",
    "        return output_path\n",
    "\n",
    "    # File path\n",
    "    combined_codes = \"Combined Codes.txt\"\n",
    "\n",
    "    # Function calls\n",
    "    Converted_Acc_Codes = combine_txt_files(combined_codes)\n",
    "\n",
    "\n",
    "    ## STEP 3: Create Unique Gene List ##\n",
    "\n",
    "    # Function to separate the accession codes in the 4th column of the CSV file\n",
    "    def separate_codes(file_path):\n",
    "        all_rows = []\n",
    "\n",
    "        # Read the input CSV file\n",
    "        with open(file_path, \"r\", newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            # Write the header\n",
    "            header = next(reader)\n",
    "            all_rows.append(header)\n",
    "\n",
    "            # Iterate through each row\n",
    "            for row in reader:\n",
    "                if len(row) > 4:  # Ensure the row has at least 4 columns\n",
    "                    # Split the 4th column at the commas\n",
    "                    entries = row[4].split(\",\")\n",
    "\n",
    "                    # Write a new row for each spliced entry\n",
    "                    for entry in entries:\n",
    "                        new_row = row[:4] + [entry.strip()] + row[5:]\n",
    "                        all_rows.append(new_row)\n",
    "\n",
    "        # Overwrite the original file with the updated data\n",
    "        with open(file_path, \"w\", newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(all_rows)\n",
    "\n",
    "    # Function to update the CSV file with the Ensembl IDs from the TXT file\n",
    "    def update_csv_with_txt(csv_file_path, txt_file_path):\n",
    "        txt_data = {}\n",
    "\n",
    "        # Load data from the TXT file into a dictionary\n",
    "        with open(txt_file_path, \"r\", encoding='utf-8') as txt_file:\n",
    "            txt_reader = csv.reader(txt_file, delimiter='\\t')  # Specify tab-separated file\n",
    "            for row in txt_reader:\n",
    "                if len(row) >= 2:\n",
    "                    key = row[0].strip()\n",
    "                    value = row[1].strip()\n",
    "                    if key in txt_data:\n",
    "                        txt_data[key].append(value)\n",
    "                    else:\n",
    "                        txt_data[key] = [value]\n",
    "\n",
    "        # Read and update the CSV file\n",
    "        updated_rows = []\n",
    "        with open(csv_file_path, \"r\", encoding='utf-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            headers = next(csv_reader)\n",
    "            headers.append(\"Ensembl ID(s)\")\n",
    "            updated_rows.append(headers)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                if len(row) >= 5 and row[4].strip():\n",
    "                    matching_key = row[4].strip()\n",
    "                    if matching_key in txt_data:\n",
    "                        # Append the new data as a new column\n",
    "                        row.append(', '.join(txt_data[matching_key]))\n",
    "                    else:\n",
    "                        # Append an empty string if no match is found\n",
    "                        row.append('')\n",
    "                else:\n",
    "                    # Append an empty string if row doesn't meet the condition\n",
    "                    row.append('')\n",
    "                # Append the updated row to the updated_rows array\n",
    "                updated_rows.append(row)\n",
    "\n",
    "        # Write updated rows back to the CSV file\n",
    "        with open(csv_file_path, \"w\", newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerows(updated_rows)\n",
    "\n",
    "        print(\"\\nStep 3 (~10 sec): \\nEnsembl IDs successfully appended to file\")\n",
    "\n",
    "    # Function to generate and save the unique Ensembl ID gene list\n",
    "    def generate_and_save_unique_gene_list(csv_file_path, output_csv_path):\n",
    "        all_entries = set()\n",
    "\n",
    "        # Read entries from the CSV file\n",
    "        with open(csv_file_path, \"r\", encoding='utf-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            headers = next(csv_reader)\n",
    "\n",
    "            for row in csv_reader:\n",
    "              if len(row) >= 5:  # Ensure the row has at least 6 columns\n",
    "                  # Split entries in the sixth column by commas\n",
    "                  entries = row[5].split(',')\n",
    "\n",
    "                  # Process each entry\n",
    "                  for entry in entries:\n",
    "                      trimmed_entry = entry.strip()  # Remove spaces before or after entry\n",
    "                      if trimmed_entry:  # Only add non-empty entries\n",
    "                        all_entries.add(trimmed_entry)\n",
    "\n",
    "        # Write unique entries to the output CSV file\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as output_csv:\n",
    "            csv_writer = csv.writer(output_csv)\n",
    "            for entry in sorted(all_entries):\n",
    "                csv_writer.writerow([entry])\n",
    "\n",
    "        # Download file if requested\n",
    "        if download_unique_list == True:\n",
    "          files.download(output_csv_path)\n",
    "\n",
    "        print(f\"Unique gene list created. There are a total of {len(all_entries)} unique Ensembl IDs\")\n",
    "        return all_entries, output_csv_path\n",
    "\n",
    "    # File path\n",
    "    output_csv_path = f\"/tmp/Unique_Gene_List_{source_organism}_{cofactor_name}.csv\"  # Temporary download\n",
    "\n",
    "    # Function calls\n",
    "    separate_codes(combined_pdb_file)\n",
    "    update_csv_with_txt(combined_pdb_file, combined_codes)\n",
    "    ensembl_id_list, ensembl_id_file = generate_and_save_unique_gene_list(combined_pdb_file, output_csv_path)\n",
    "\n",
    "\n",
    "    ## STEP 4: CPDB Database and Variable Retrieval ##\n",
    "\n",
    "    # Function to download the appropriate database for each organism from CPDB\n",
    "    def download_CPDB_pathways(source_organism):\n",
    "      # Define URL for each source organism\n",
    "      if \"Homo sapiens\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/CPDB/daccess\"\n",
    "      elif \"Mus musculus\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/MCPDB/daccess\"\n",
    "      elif \"Saccharomyces cerevisiae\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/YCPDB/daccess\"\n",
    "\n",
    "      # Make GET request\n",
    "      response = requests.get(url_CPDB)\n",
    "\n",
    "      # Check if the request was successful (status code = 200)\n",
    "      if response.status_code == 200:\n",
    "        download_dir = \"/tmp\"  # Temporary download directory\n",
    "\n",
    "        # Clear the download directory before starting\n",
    "        for f in os.listdir(download_dir):\n",
    "          file_path = os.path.join(download_dir, f)\n",
    "          try:\n",
    "              if os.path.isfile(file_path) and not f.lower().endswith(\".csv\"):  # Don't clear the CSV file from generate_and_save_unique_gene_list\n",
    "                  os.remove(file_path)\n",
    "          except Exception as e:\n",
    "              print(f\"Error deleting file {file_path} from temporary download directory: {e}\")\n",
    "\n",
    "        # Define options for how Selenium will access the website\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "        options.add_argument(\"--disable-gpu\")  # For Linux compatibility\n",
    "        options.add_argument(\"--no-sandbox\")  # Needed in Colab\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")  # Prevents memory issues\n",
    "\n",
    "        # Set Edge preferences for downloading files automatically\n",
    "        edge_prefs = {\n",
    "            \"download.default_directory\": download_dir,\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", edge_prefs)\n",
    "\n",
    "        # Specify the executable path and create the Edge Webdriver instance\n",
    "        edge_service = EdgeService(executable_path='/usr/bin/edgedriver')\n",
    "        driver = webdriver.Edge(service=edge_service, options=options)\n",
    "        driver.get(url_CPDB)\n",
    "\n",
    "        # Select Ensembl ID as the gene identifier\n",
    "        identifier_box = driver.find_element(By.NAME, \"idtype\")\n",
    "        select = Select(identifier_box)\n",
    "        select.select_by_value(\"ensembl\")\n",
    "\n",
    "        # Wait for the database download button to be identified and click it\n",
    "        download_button = driver.find_element(By.XPATH, \"//span[@class='pseudolink' and contains(text(),'accession numbers')]\")\n",
    "        download_button.click()\n",
    "\n",
    "        # Wait for the file to download\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Check if the file is properly downloaded\n",
    "        downloaded_files = os.listdir(download_dir)\n",
    "\n",
    "        # If multiple files exist, filter for the desired file type\n",
    "        desired_file = [f for f in downloaded_files if f.endswith(\".tab\")]\n",
    "        format_desired_file = desired_file\n",
    "        print(f\"\\nStep 4 (~15 sec): \\nConsenus Path DB database succesfully saved as '{','.join(format_desired_file)}'\")\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        return f\"/tmp/{desired_file[0]}\"\n",
    "      # Site not available error\n",
    "      elif response.status_code == 503:\n",
    "        print(f\"Error accessing {url_CPDB} for data download. This site is temporarily unavailable due to maintenance downtime or capacity problems. \\nPlease select a different organism or try again later.\")\n",
    "        print(response.status_code, response.text)\n",
    "\n",
    "      # Other error accessing the request\n",
    "      else:\n",
    "          print(f\"Error accessing {url_CPDB} for data download:\\n\", response.status_code, response.text)\n",
    "          sys.exit()  # Force program to exit\n",
    "\n",
    "    # Function to convert the .tab CPDB database file to a .csv file\n",
    "    def tab_file_to_csv(input_file, output_file):\n",
    "      # Read the tab-delimited .tab file\n",
    "      df = pd.read_csv(input_file, delimiter='\\t')\n",
    "\n",
    "      # Modify the 'ensembl_ids' column to add a space after each comma\n",
    "      df['ensembl_ids'] = df['ensembl_ids'].apply(lambda x: ', '.join(x.split(',')) if isinstance(x, str) else x)\n",
    "\n",
    "      # Write the data to a .csv file\n",
    "      df.to_csv(output_file, index=False)\n",
    "\n",
    "      print(f\"Consenus Path DB tab file successfully converted to csv file\")\n",
    "\n",
    "    # Function to retrieve the M and N variable needed for the overrepresentation analysis from CPDB\n",
    "    def retrieve_variable_from_CPDB(source_organism, ensembl_id_file):\n",
    "      # Define URL for each source organism\n",
    "      if \"Homo sapiens\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/CPDB/fct_annot\"\n",
    "      elif \"Mus musculus\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/MCPDB/fct_annot\"\n",
    "      elif \"Saccharomyces cerevisiae\" in source_organism:\n",
    "        url_CPDB = \"http://cpdb.molgen.mpg.de/YCPDB/fct_annot\"\n",
    "\n",
    "      # Make GET request\n",
    "      response = requests.get(url_CPDB)\n",
    "\n",
    "      # Check if the request was successful (status code = 200)\n",
    "      if response.status_code == 200:\n",
    "          # Define options for how Selenium will access the website\n",
    "          options = Options()\n",
    "          options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "          options.add_argument(\"--disable-gpu\")  # For Linux compatibility\n",
    "          options.add_argument(\"--no-sandbox\")  # Needed in Colab\n",
    "          options.add_argument(\"--disable-dev-shm-usage\")  # Prevents memory issues\n",
    "\n",
    "          # Specify the executable path and create the Edge Webdriver instance\n",
    "          edge_service = EdgeService(executable_path='/usr/bin/edgedriver')\n",
    "          driver = webdriver.Edge(service=edge_service, options=options)\n",
    "          driver.get(url_CPDB)\n",
    "\n",
    "          # Identify the button to upload a gene list and upload the unique gene list file\n",
    "          gene_list_box = driver.find_element(By.NAME, \"fh\")\n",
    "          gene_list_box.send_keys(ensembl_id_file)\n",
    "\n",
    "          # Identify the ID type dropdown and select the \"Ensembl gene\" option\n",
    "          type_dropdown = driver.find_element(By.NAME, \"idtype\")\n",
    "          select_type = Select(type_dropdown)\n",
    "          select_type.select_by_value(\"ensembl\")\n",
    "\n",
    "          # Identify and click the continue button\n",
    "          continue_button_1 = driver.find_element(By.CLASS_NAME, 'cpdbutton')\n",
    "          continue_button_1.click()\n",
    "\n",
    "          # Wait until the checkbox is present\n",
    "          wait = WebDriverWait(driver, 10)\n",
    "\n",
    "          # Identify and click the checkbox for a pathway based analysis\n",
    "          checkbox = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@type='checkbox' and @value='pwy']\")))\n",
    "          checkbox.click()\n",
    "\n",
    "          # Identify and click the continue button\n",
    "          continue_button_2 = driver.find_element(By.CLASS_NAME, 'cpdbutton')\n",
    "          continue_button_2.click()\n",
    "\n",
    "          try:\n",
    "            # Wait for the next page to load and find the needed variables\n",
    "            mapped_entities = wait.until(EC.presence_of_element_located((By.XPATH, '//a[@href=\"showTranslation\" and normalize-space(text())!=\"mapped entities:\" and number(text())=number(text())]')))\n",
    "            variables = driver.find_elements(By.XPATH, \"//b[contains(text(),'')]\")  # Extract the numbers from the <b> tag\n",
    "\n",
    "            # Convert the text to integers\n",
    "            M_variable = int(variables[0].text)\n",
    "            N_variable = int(variables[1].text)\n",
    "            mapped_entities = int(mapped_entities.text)\n",
    "\n",
    "            print(f\"{mapped_entities} genes from the Ensembl ID list are identifiable by CPDB\")\n",
    "            print(f\"{M_variable} genes from the Ensembl ID list are present in at least one CPDB pathway\")\n",
    "            print(f\"{N_variable} genes identifiable by Ensembl ID and present in at least one CPDB pathway\")\n",
    "\n",
    "            # Close the browser\n",
    "            driver.quit()\n",
    "\n",
    "            return M_variable, N_variable\n",
    "          except:\n",
    "            print(\"\\nERROR: The program has exited as there are not enough overrepresented pathways to perform hierarchical clustering. Try selecting different values\")\n",
    "            driver.quit()  # Ensures the browser closes properly if needed\n",
    "            return None, None\n",
    "\n",
    "      # Error accessing the request\n",
    "      else:\n",
    "          print(f\"Error accessing {url_CPDB} for data download:\", response.status_code, response.text)\n",
    "          sys.exit()  # Force program to exit\n",
    "\n",
    "    # File path\n",
    "    CPDB_database_converted_file = 'CPDB_database_converted_file.csv'\n",
    "\n",
    "    # Function calls\n",
    "    CPDB_database_tab_file = download_CPDB_pathways(source_organism)\n",
    "    tab_file_to_csv(CPDB_database_tab_file, CPDB_database_converted_file)\n",
    "    M_variable, N_variable = retrieve_variable_from_CPDB(source_organism, ensembl_id_file)\n",
    "\n",
    "    if M_variable == None and N_variable == None:\n",
    "      return\n",
    "\n",
    "\n",
    "    ## STEP 5: Overrepresentation Analysis ##\n",
    "\n",
    "    # Function to handle duplicate pathway names by appending (#) and to handle gene names matching pathway names by appending \"Pathway\"\n",
    "    def handle_duplicate_pathway_names(rows):\n",
    "        pathway_counts = {}\n",
    "        updated_rows = []\n",
    "\n",
    "        # Extract all gene names from column 4\n",
    "        all_genes = set()\n",
    "        for row in rows:\n",
    "            if len(row) > 3:\n",
    "                all_genes.update(row[3].split(\",\"))  # Collect all gene names\n",
    "\n",
    "        # Iterate over each row in the input\n",
    "        for row in rows:\n",
    "            pathway = row[0]  # Extract the pathway name from the first column\n",
    "\n",
    "            # If the pathway name matches a gene name, append \"Pathway\"\n",
    "            if pathway in all_genes:\n",
    "                pathway += \" Pathway\"\n",
    "\n",
    "            # Ensure unique pathway names\n",
    "            if pathway in pathway_counts:\n",
    "                pathway_counts[pathway] += 1\n",
    "                pathway = f\"{pathway} ({pathway_counts[pathway]})\"\n",
    "            else:\n",
    "                pathway_counts[pathway] = 0\n",
    "\n",
    "            # Update the pathway name in the row\n",
    "            row[0] = pathway\n",
    "            updated_rows.append(row)\n",
    "\n",
    "        return updated_rows\n",
    "\n",
    "    # Function to update the CSV file with the P and Q values for each pathway\n",
    "    def calculate_p_and_q_values(CPDB_database_converted_file, ensembl_id_list, all_P_and_Q, M_variable, N_variable):\n",
    "        # Read the comparison values (Ensembl ID list) into a set for quick lookup\n",
    "        comparison_values = set(entry.strip() for entry in ensembl_id_list)  # .strip() to make the search space insensitive\n",
    "\n",
    "        # Prepare to store rows and p-values\n",
    "        rows = []\n",
    "        p_value_array = []\n",
    "\n",
    "        # Process the input file\n",
    "        with open(CPDB_database_converted_file, \"r\") as in_file:\n",
    "            reader = csv.reader(in_file)\n",
    "\n",
    "            # Read the header\n",
    "            header = next(reader)\n",
    "            if len(header) < 7:\n",
    "                header += [\"Match Percentage\", \"P-Value\", \"Q-Value\"]  # Add additional header columns\n",
    "\n",
    "            # Iterate over each row\n",
    "            for row in reader:\n",
    "                # Ensure the row has at least 4 columns\n",
    "                if len(row) > 3:\n",
    "                    input_values = row[3].split(\",\") # Comma separate the values\n",
    "\n",
    "                    # Find matches between input values and comparison values\n",
    "                    matching_values = [value.strip() for value in input_values if value.strip() in comparison_values]\n",
    "                    row[3] = \", \".join(matching_values)\n",
    "\n",
    "                    # Calculate percentage of genes matching between the total genes in the pathway and the genes in the comparison set\n",
    "                    total_count = len(input_values)\n",
    "                    matching_count = len(matching_values)\n",
    "                    match_percentage = (matching_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "                    # Set the parameters for the hypergeom function\n",
    "                    N = N_variable  # Total background size (total number of genes identifiable by Ensembl ID and present in at least one ConsensusPathDB pathway)\n",
    "                    n = total_count  # Total genes in specific pathway\n",
    "                    M = M_variable  # Mapped entities (genes from the input list present in at least one ConsensusPathDB pathway)\n",
    "                    x = matching_count  # Matching genes between specific pathway and comparison file\n",
    "\n",
    "                    # Calculate the p-value\n",
    "                    p_value = hypergeom.sf(x - 1, N, n, M)   # Tests whether the overlap between the input gene set and the specific pathway is greater than what would be expected by random chance (smaller p value = more statistically significant)\n",
    "\n",
    "                    # Append match percentage and p-value to the rows of data\n",
    "                    if len(row) < 6:\n",
    "                        row += [f\"{matching_count}/{total_count} = {match_percentage:.2f}%\", f\"{p_value:.2e}\"]\n",
    "                    else:\n",
    "                        row[4] = f\"{matching_count}/{total_count} = {match_percentage:.2f}%\"\n",
    "                        row[5] = f\"{p_value:.2e}\"\n",
    "\n",
    "                    # Store the row and p-value\n",
    "                    rows.append(row)\n",
    "                    p_value_array.append(p_value)\n",
    "                else:\n",
    "                    if len(row) < 6:\n",
    "                        row += [\"0.00%\", \"\"]\n",
    "                    rows.append(row)\n",
    "\n",
    "        # Function call\n",
    "        rows = handle_duplicate_pathway_names(rows)\n",
    "\n",
    "        # Calculate q-values using the Bonferroni correction\n",
    "        _, q_values, _, _ = multipletests(p_value_array, method=\"bonferroni\")\n",
    "\n",
    "        # Append the q-values to the rows of data and write the rows to the output file\n",
    "        with open(all_P_and_Q, \"w\", newline=\"\") as out_file:\n",
    "            writer = csv.writer(out_file)\n",
    "            writer.writerow(header)  # Write header\n",
    "            for row, q_val in zip(rows, q_values):  # Combines rows and q_values into a single iterator of tuples\n",
    "                row.append(f\"{q_val:.2e}\")  # Append q-value\n",
    "                writer.writerow(row)\n",
    "        print(\"\\nStep 5 (~8 sec): \\nP and Q values from overrepresentation analysis successfully appended to file\")\n",
    "\n",
    "    # Function to filter rows based on a threshold q-value\n",
    "    def filter_by_q_value(all_P_and_Q, filtered_P_and_Q_file, threshold_q):\n",
    "        num_rows = 0  # Tracks the number of pathways that are within the threshold\n",
    "\n",
    "        # Process the input and output files\n",
    "        with open(all_P_and_Q, \"r\") as in_file, open(filtered_P_and_Q_file, \"w\", newline=\"\") as out_file:\n",
    "            reader = csv.reader(in_file)\n",
    "            writer = csv.writer(out_file)\n",
    "\n",
    "            # Write the header\n",
    "            header = next(reader)\n",
    "            writer.writerow(header)\n",
    "\n",
    "            # Filter rows based on q-value threshold\n",
    "            for row in reader:\n",
    "                if len(row) > 6 and float(row[6]) <= threshold_q:  # Check if q-value is within threshold\n",
    "                    num_rows += 1\n",
    "                    writer.writerow(row)  # If it is, write it to the output file\n",
    "\n",
    "        print(f\"P and Q values successfully filtered by threshold Q value of {threshold_q}\")\n",
    "        print(f\"There are {num_rows} pathways with a Q value less than {threshold_q}\")\n",
    "\n",
    "        if num_rows <= 1:\n",
    "          print(\"\\nERROR: The program has exited as there are not enough overrepresented pathways to perform hierarchical clustering. Try selecting a larger Q-value\")\n",
    "          return True\n",
    "        else:\n",
    "          return False\n",
    "\n",
    "    #  File paths\n",
    "    all_P_and_Q = \"all_P_and_Q.csv\"\n",
    "    filtered_P_and_Q_file = f\"filtered_P_and_Q_file_{source_organism}_{cofactor_name}.csv\"\n",
    "\n",
    "    # Function calls\n",
    "    calculate_p_and_q_values(CPDB_database_converted_file, ensembl_id_list, all_P_and_Q, M_variable, N_variable)\n",
    "    step5_flag = filter_by_q_value(all_P_and_Q, filtered_P_and_Q_file, threshold_q_value)\n",
    "\n",
    "    # Exits program if not enough pathways\n",
    "    if step5_flag:\n",
    "      return\n",
    "\n",
    "    # Download file if requested\n",
    "    if download_filtered_overrep == True:\n",
    "      files.download(filtered_P_and_Q_file)\n",
    "\n",
    "\n",
    "    ## STEP 6: Hierarchical Clustering and Cluster Analysis Output ##\n",
    "\n",
    "    # Function to create the heatmap (based on number of clusters)\n",
    "    def color_map(num_clusters, compactness):\n",
    "      # Used to help determine the max value on the scale\n",
    "      scale_factor = (max(max(sublist) for sublist in num_clusters if sublist)) - (min(min(sublist) for sublist in num_clusters if sublist))\n",
    "\n",
    "      # Colour range\n",
    "      max_value = math.ceil(((max(max(sublist) for sublist in num_clusters if sublist))/2) - (scale_factor*0.1))\n",
    "      if max_value == 1:\n",
    "        max_value = 2\n",
    "        num_ticks = 2  # Number of tick marks\n",
    "      elif max_value <= 5 and max_value > 1:\n",
    "        num_ticks = 3  # Number of tick marks\n",
    "      else:\n",
    "        num_ticks = 6  # Number of tick marks\n",
    "\n",
    "      # Define the dimensions and data range\n",
    "      x_labels = range(1, 9)  # X-axis labels from 0 to 8\n",
    "      y_labels = [round(i * 0.05, 2) for i in range(1, 20)]  # Y-axis labels from 0 to 0.95 in increments of 0.05\n",
    "\n",
    "      avg_compactness = np.array(compactness) # Array of compactness values\n",
    "      data = np.array(num_clusters) # Array of num_clusters values\n",
    "      clipped_data = np.clip(data, 1, max_value)  # Sets any value above the max_value to the max\n",
    "\n",
    "      # Normalize the data to a range of 0 to 1 for the color mapping\n",
    "      normalized_data = (clipped_data - 1) / (max_value - 1)\n",
    "\n",
    "      # Create the figure and axis\n",
    "      fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "      # Plot the data as a colored grid\n",
    "      cmap = plt.colormaps.get_cmap('coolwarm')  # Blue to red gradient\n",
    "      heatmap = ax.imshow(normalized_data, cmap=cmap, origin='lower', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "      # Add text annotations in the cells\n",
    "      for i in range(data.shape[0]):  # Iterate over rows\n",
    "          for j in range(data.shape[1]):  # Iterate over columns\n",
    "            # Combine the number of clusters and the average compactness into one string\n",
    "            cell_text = f\"{int(data[i, j])}\\n{avg_compactness[i, j]}\"\n",
    "\n",
    "            # Add the combined text to the heatmap\n",
    "            ax.text(j, i, cell_text, ha='center', va='center', color='black', fontsize=9)\n",
    "\n",
    "      # Add color bar to show the scale\n",
    "      cbar = plt.colorbar(heatmap, ax=ax) # Plot the color bar\n",
    "      cbar.set_label('Number of Clusters', rotation=270, labelpad=15) # Set color bar ticks to include max_value\n",
    "      tick_values = np.linspace(1, max_value, num=num_ticks)  # Generate evenly spaced ticks from 1 to max_value\n",
    "      cbar.set_ticks((tick_values - 1) / (max_value - 1))  # Normalize tick positions to 0-1 scale\n",
    "      cbar.set_ticklabels([int(tick) if tick != max(tick_values) else f\"{int(tick)}+\" for tick in tick_values])  # Set tick labels to integer values\n",
    "\n",
    "      # Set axis labels and title\n",
    "      ax.set_xticks(range(len(x_labels)))\n",
    "      ax.set_xticklabels(x_labels)\n",
    "      ax.set_yticks(range(len(y_labels)))\n",
    "      ax.set_yticklabels(y_labels)\n",
    "      ax.set_xlabel('Number of Pathways in Cluster')\n",
    "      ax.set_ylabel('Cutoff Value')\n",
    "\n",
    "      # Add grid lines\n",
    "      ax.set_xticks(np.arange(-0.5, len(x_labels), 1), minor=True)\n",
    "      ax.set_yticks(np.arange(-0.5, len(y_labels), 1), minor=True)\n",
    "      ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "      ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "      # Show the plot\n",
    "      print(\"Top value in colour map is the number of clusters. The bottom value is the average compactness.\")\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "    # Function to write number of clusters, min number of pathways, cutoff value, average compactness, % pathways, % genes, and cluster labels to the output CSV file\n",
    "    def write_clusters_to_csv(num_clusters, compactness, output_file, percent_paths, percent_genes, labels):\n",
    "        x_labels = range(1, 9)  # Min Number of Pathways in Cluster (X-axis)\n",
    "        y_labels = [round(i * 0.05, 2) for i in range(1, 20)]  # Cutoff Values (Y-axis)\n",
    "\n",
    "        with open(output_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write header\n",
    "            writer.writerow([\"Number of Clusters\", \"Min Number of Pathways in Cluster\", \"Cutoff Value\", \"Average Compactness\"])\n",
    "\n",
    "            # Iterate through data\n",
    "            for i, cutoff_value in enumerate(y_labels):\n",
    "                for j, pathways in enumerate(x_labels):\n",
    "                    num_cluster_value = num_clusters[i][j]\n",
    "                    compactness_value = compactness[i][j]\n",
    "\n",
    "                    writer.writerow([num_cluster_value, pathways, cutoff_value, compactness_value])  # Write values to the output file\n",
    "\n",
    "        with open(output_file, \"r\", newline=\"\") as file:\n",
    "            reader = list(csv.reader(file))\n",
    "\n",
    "        # Modify header to add new column names if not already present\n",
    "        if len(reader) > 0:\n",
    "            header = reader[0]\n",
    "            header += [\"% Pathways\", \"% Genes\", \"Cluster Labels\"]\n",
    "\n",
    "        # Update rows with corresponding values\n",
    "        for i, row in enumerate(reader[1:]):  # Skip header\n",
    "            if i < len(percent_paths):  # Avoid index errors\n",
    "                row += [percent_paths[i], percent_genes[i], labels[i]]\n",
    "\n",
    "        # Write updated content back to the file\n",
    "        with open(output_file, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(reader)  # Write modified rows back\n",
    "\n",
    "        print(f\"Data successfully written to '{output_file}'\")\n",
    "\n",
    "    # Function to determine how closely related the pathways in a cluster are (1 = very related, 0 = not related)\n",
    "    def compute_intra_cluster_compactness(clusters, similarity_matrix):\n",
    "        compactness_scores = {}\n",
    "\n",
    "        for cluster_label, pathway_indices in clusters.items():\n",
    "            # Generate all pairs of pathways within the cluster\n",
    "            pairs = list(combinations(pathway_indices, 2))\n",
    "\n",
    "            if not pairs:  # Single pathway in the cluster\n",
    "                compactness_scores[cluster_label] = 1.0  # Max compactness for single element\n",
    "                continue\n",
    "\n",
    "            # Compute the total similarity by summing up pairwise similarities within the cluster\n",
    "            total_similarity = sum(similarity_matrix[i, j] for i, j in pairs)\n",
    "            # Compute the average similarity (compactness) for this cluster\n",
    "            compactness_scores[cluster_label] = total_similarity / len(pairs)\n",
    "\n",
    "        # Calculate overall average compactness across all clusters\n",
    "        if len(compactness_scores) == 0:\n",
    "          total_compactness = 0\n",
    "        else:\n",
    "          total_compactness = round(sum(compactness_scores.values()) / len(compactness_scores), 3)\n",
    "\n",
    "        return total_compactness\n",
    "\n",
    "    # Function to compute similarity between two gene lists\n",
    "    def compute_similarity(set1, set2):\n",
    "        intersection = set1.intersection(set2)  # Number of genes matching between the pathways\n",
    "        union = set1.union(set2)  # Total number of genes in both the pathways (without duplicates)\n",
    "        return len(intersection) / len(union) if union else 0\n",
    "\n",
    "    # Function to create a similarity matrix (percentage of genes matching between two pathways) from the CSV file\n",
    "    def create_similarity_matrix(input_file):\n",
    "        total_genes = set()\n",
    "\n",
    "        with open(input_file, \"r\") as file:\n",
    "            reader = list(csv.reader(file))\n",
    "            headers = reader[0]  # First row is the header\n",
    "            data = reader[1:]  # Exclude the header\n",
    "\n",
    "            # Extract pathway names and gene sets\n",
    "            pathway_names = [row[0] for row in data]  # Extracts the pathway names from the first column\n",
    "            gene_sets = [set(gene.strip() for gene in row[3].split(\",\")) for row in data]  # Extracts the genes from the 4th column and comma separates them, .strip() makes it space insensitive\n",
    "\n",
    "            # Initialize similarity matrix\n",
    "            n = len(data)\n",
    "            similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "            # Compute pairwise similarity\n",
    "            for i in range(n):\n",
    "                total_genes = total_genes.union(gene_sets[i]) # Extracts all the genes in all pathways (without duplicates)\n",
    "                for j in range(n):\n",
    "                    similarity_matrix[i, j] = compute_similarity(gene_sets[i], gene_sets[j])  # Function call\n",
    "\n",
    "            print(\"\\nStep 6 (~15 sec): \\nSimilarity matrix created\")\n",
    "            return similarity_matrix, pathway_names, total_genes, gene_sets\n",
    "\n",
    "    # Function to perform hierarchical clustering\n",
    "    def hierarchical_clustering(similarity_matrix, pathway_names, total_genes, gene_sets):\n",
    "        # Convert similarity matrix to a distance matrix (percentage of genes that differ)\n",
    "        distance_matrix = squareform(1 - similarity_matrix)\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        linkage_matrix = linkage(distance_matrix, method=\"average\")\n",
    "\n",
    "        # Initalize arrays for the values to be written to the output file\n",
    "        percent_paths = []\n",
    "        percent_genes = []\n",
    "        labels = []\n",
    "\n",
    "        num_clusters = [[] for _ in range(19)]  # Initialize sublists (one for each value of i)\n",
    "        compactness = [[] for _ in range(19)] # Initialize sublists (one for each value of i)\n",
    "\n",
    "        for i in range(1, 20):\n",
    "          # Form clusters\n",
    "          cutoff_value = round(i * 0.05, 2) # Maximum allowed difference between pathways in a cluster\n",
    "          cluster_labels = fcluster(linkage_matrix, cutoff_value, criterion=\"distance\") # Forms flat clusters from hierarchical clustering\n",
    "\n",
    "          # Organize pathways into clusters storing pathway names\n",
    "          clusters_names = {}\n",
    "          for pathway, label in zip(pathway_names, cluster_labels):\n",
    "              if label not in clusters_names:\n",
    "                  clusters_names[label] = []  # Initialize an empty list for this cluster\n",
    "              clusters_names[label].append(pathway) # Add the pathway to its corresponding cluster\n",
    "\n",
    "          # Iterate over different minimum pathway counts (1 to 8) to filter clusters\n",
    "          for num_pathways in range(1, 9):\n",
    "            filtered_clusters_names = {key: value for key, value in clusters_names.items() if len(value) >= num_pathways} # Keep only clusters that have at least num_pathways pathways\n",
    "            num_clusters[i-1].append(len(filtered_clusters_names))  # Store these clusters\n",
    "\n",
    "            with open(filtered_P_and_Q_file, \"r\") as in_file:\n",
    "              reader = list(csv.reader(in_file))\n",
    "              pathways_expressed = 0\n",
    "              data = reader[1:]\n",
    "              unique_genes = set()\n",
    "\n",
    "              for cluster_id, pathway_list in filtered_clusters_names.items():\n",
    "                    pathways_expressed += len(pathway_list)  # Tracks the number of pathways written to the file\n",
    "\n",
    "                    for pathway in pathway_list:\n",
    "                      unique_genes = unique_genes.union(gene_sets[[row[0] for row in data].index(pathway)]) # Tracks the genes that belong to the pathways written to the file (without duplicates)\n",
    "\n",
    "              percent_paths.append(round((pathways_expressed/len(data))*100, 2))  # Calculate the percentage of pathways expressed\n",
    "              percent_genes.append(round((len(unique_genes)/len(total_genes))*100, 2))  # Calculate the percentage of genes expressed\n",
    "              labels.append(word_frequency(filtered_clusters_names))  # Find the word frequency for the clusters in the file\n",
    "\n",
    "          # Organize pathways into clusters but store pathway indices\n",
    "          clusters_index = {}\n",
    "          for idx, label in enumerate(cluster_labels):\n",
    "            if label not in clusters_index:\n",
    "                clusters_index[label] = []  # Initialize an empty list for this cluster\n",
    "            clusters_index[label].append(idx)  # Add the index of the pathway to the cluster\n",
    "\n",
    "          # Iterate over different minimum pathway counts (1 to 8) to filter clusters\n",
    "          for num_pathways in range(1, 9):\n",
    "            filtered_clusters_index = {key: value for key, value in clusters_index.items() if len(value) >= num_pathways} # Keep only clusters that have at least num_pathways pathways\n",
    "            compactness[i-1].append(compute_intra_cluster_compactness(filtered_clusters_index, similarity_matrix))  # Compute and store the compactness score for the filtered clusters\n",
    "\n",
    "        print(\"Hierarchical clustering completed\")\n",
    "        color_map(num_clusters, compactness) # Run the graphing function\n",
    "        write_clusters_to_csv(num_clusters, compactness, cluster_file, percent_paths, percent_genes, labels)  # Write the results to a CSV file\n",
    "\n",
    "    # Function to return the highest frequency word(s) from each cluster\n",
    "    def word_frequency(cluster_names):\n",
    "        # Common filler words or other unhelpful naming words\n",
    "        common_words = {\"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"it\", \"for\", \"on\", \"with\", \"as\", \"was\", \"at\", \"by\", \"an\", \"homo\", \"sapiens\", \"human\", \"humans\",\n",
    "                        \"saccharomyces\", \"cerevisiae\", \"budding\", \"yeast\", \"mus\", \"musculus\", \"mouse\", \"mice\", \"between\"}\n",
    "\n",
    "        rows = [[] for _ in range(len(cluster_names))]\n",
    "        row_number = 0\n",
    "\n",
    "        # Iterate through the pathway names in each cluster\n",
    "        for id, pathway_list in cluster_names.items():\n",
    "          row_number += 1\n",
    "          cluster = \", \".join(pathway for pathway in pathway_list)\n",
    "\n",
    "          words = re.findall(r'\\b\\w+(?:-\\w+)*\\b', cluster.lower())  # Extract words and convert to lowercase (treats hyphenated words as one word)\n",
    "          words = [word for word in words if word not in common_words]\n",
    "          word_counts = Counter(words)  # Count word frequencies\n",
    "\n",
    "          if word_counts:\n",
    "              max_frequency = max(word_counts.values())  # Find the highest word frequency value\n",
    "              most_frequent_words = [word for word, count in word_counts.items() if count == max_frequency]  # Store the word(s) with this frequency value\n",
    "              rows[row_number-1] = most_frequent_words\n",
    "\n",
    "        sorted_rows = sorted(rows, key=lambda x: x[0])  # Sort alphabetically\n",
    "        formatted_output = \"; \".join([\", \".join(row) for row in sorted_rows if row])  # Join words from the same cluster with ',' and join the different clusters with ';'\n",
    "\n",
    "        return formatted_output\n",
    "\n",
    "    # Main function call\n",
    "    def main(filtered_P_and_Q_file):\n",
    "        # Step 6.1: Create the similarity matrix\n",
    "        similarity_matrix, pathway_names, total_genes, gene_sets = create_similarity_matrix(filtered_P_and_Q_file)\n",
    "\n",
    "        # Step 6.2: Perform hierarchical clustering\n",
    "        hierarchical_clustering(similarity_matrix, pathway_names, total_genes, gene_sets)\n",
    "\n",
    "    # File path\n",
    "    cluster_file = f\"Cluster_Analysis_{source_organism}_{cofactor_name}.csv\"\n",
    "\n",
    "    # Main function call\n",
    "    main(filtered_P_and_Q_file)\n",
    "\n",
    "    # Download the file if requested\n",
    "    if download_final_cluster_analysis == True:\n",
    "      files.download(cluster_file)\n",
    "      print(f\"Final cluster analysis file downloaded as '{cluster_file}'\")\n",
    "\n",
    "    print(\"\\nTo download a specific set, start the next code block\")\n",
    "\n",
    "# Create the first (default) polymer type and cofactor that can't be removed\n",
    "create_polymer_entry(index=1, removable=False)\n",
    "create_cofactor_entry(index=1, removable=False)\n",
    "\n",
    "# Attach event listeners\n",
    "add_button_p.on_click(add_polymer_entry)\n",
    "remove_button_p.on_click(remove_polymer_entry)\n",
    "and_or_button_p.on_click(and_or_click)\n",
    "\n",
    "add_button_c.on_click(add_cofactor_entry)\n",
    "remove_button_c.on_click(remove_cofactor_entry)\n",
    "and_or_button_c.on_click(and_or_click)\n",
    "\n",
    "submit_button.on_click(on_button_click)\n",
    "\n",
    "# Display all widgets\n",
    "display(row1, add_remove_widgets_p, polymer_entries_container, add_remove_widgets_c, cofactor_entries_container, row2, row3, row4, row5, row6, submit_button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfQHgS_zSg7d"
   },
   "outputs": [],
   "source": [
    "#*** Secondary Program ***#\n",
    "\n",
    "## Specific Cluster File ##\n",
    "\n",
    "import csv  # To handle CSV files\n",
    "import numpy as np  # For numerical operations and matrix manipulations\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster  # For hierarchical clustering and dendrogram visualization\n",
    "from scipy.spatial.distance import squareform  # To convert a condensed distance matrix into a square form\n",
    "import matplotlib.pyplot as plt  # For creating plots\n",
    "from itertools import combinations  # Generates all possible unique pairs of elements\n",
    "from collections import Counter  # To count the occurrences of elements in an iterable\n",
    "import re  # To extract words from text\n",
    "import ipywidgets as widgets  # For interactive UI controls in Jupyter notebooks\n",
    "from IPython.display import display  # To display content\n",
    "from google.colab import files  # For file upload and download in Colab\n",
    "\n",
    "# User specified values\n",
    "cutoff_value = 0.0 # Maximum allowed difference between pathways in a cluster\n",
    "num_pathways = 0  # Minimum allowed number of pathways in a cluster\n",
    "plot_dendrogram = True\n",
    "print_gene_cluster_membership = True\n",
    "download_file = True\n",
    "\n",
    "# Define Float Input Widget\n",
    "cutoff_input = widgets.FloatText(\n",
    "    value=0.5,  # Default value\n",
    ")\n",
    "\n",
    "# Define Integer Input Widget\n",
    "pathway_input = widgets.IntText(\n",
    "    value=1,  # Default value\n",
    ")\n",
    "\n",
    "# Define True/False Dropdown Widgets\n",
    "dendrogram_TF = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=False,  # Default value\n",
    ")\n",
    "\n",
    "gene_clusters_TF = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=False,  # Default value\n",
    ")\n",
    "\n",
    "# Additional Gene Cluster Membership Widgets\n",
    "gene_clusters_operator = widgets.Dropdown(\n",
    "    options=[\"less than or equal to\", \"exactly\", \"greater than or equal to\"],\n",
    "    value=\"greater than or equal to\",\n",
    ")\n",
    "\n",
    "gene_clusters_int = widgets.IntText(\n",
    "    value=1,\n",
    ")\n",
    "\n",
    "extra_inputs_box = widgets.VBox(children=[], visible=False)  # Container to hold inputs\n",
    "\n",
    "# Download File Widget\n",
    "download_TF = widgets.Dropdown(\n",
    "    options=[(\"True\", True), (\"False\", False)],  # Maps labels to actual bool values\n",
    "    value=True,  # Default value\n",
    ")\n",
    "\n",
    "# Define Submit Button\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit\",\n",
    "    button_style=\"primary\", # Button style\n",
    ")\n",
    "\n",
    "# Create Labels (Ensuring Descriptions are Fully Visible)\n",
    "label1 = widgets.Label(\"Enter cutoff value:\")\n",
    "label2 = widgets.Label(\"Enter minimum number of pathways:\")\n",
    "label3 = widgets.Label(\"Show dendrogram:\")\n",
    "label4 = widgets.Label(\"Show gene cluster memberships:\")\n",
    "label5 = widgets.Label(\"Download specific set:\")\n",
    "label6 = widgets.Label(\"If genes in\")\n",
    "label7 = widgets.Label(\"cluster(s)\")\n",
    "\n",
    "# Use HBox to Align Labels and Widgets\n",
    "row1 = widgets.HBox([label1, cutoff_input])\n",
    "row2 = widgets.HBox([label2, pathway_input])\n",
    "row3 = widgets.HBox([label3, dendrogram_TF])\n",
    "row4 = widgets.HBox([label4, gene_clusters_TF])\n",
    "row5 = widgets.HBox([label5, download_TF])\n",
    "row6 = widgets.HBox([label6, gene_clusters_operator, gene_clusters_int, label7])\n",
    "\n",
    "# Output Display\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to show/hide inputs based on dropdown selection\n",
    "def toggle_extra_inputs(change):\n",
    "    if change['new']:  # If True is selected\n",
    "        extra_inputs_box.children = [row6]  # Add input fields\n",
    "        extra_inputs_box.layout.display = 'block'\n",
    "    else:  # If False is selected\n",
    "        extra_inputs_box.children = []  # Remove input fields\n",
    "        extra_inputs_box.layout.display = 'none'\n",
    "\n",
    "# Define Button Click Event\n",
    "def on_button_click(b):  # Run code after user inputs submitted\n",
    "    global cutoff_value, num_pathways, plot_dendrogram, print_gene_cluster_membership, download_file, filtered_P_and_Q_file\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "\n",
    "        # Set variables with user input values\n",
    "        if cutoff_input.value <= 0:\n",
    "          cutoff_input.value = 0.1\n",
    "        else:\n",
    "          cutoff_value = cutoff_input.value\n",
    "\n",
    "        if pathway_input.value <= 0:\n",
    "          num_pathways = 1\n",
    "        else:\n",
    "          num_pathways = pathway_input.value\n",
    "\n",
    "        plot_dendrogram = dendrogram_TF.value\n",
    "        print_gene_cluster_membership = gene_clusters_TF.value\n",
    "        download_file = download_TF.value\n",
    "\n",
    "        if print_gene_cluster_membership == True:\n",
    "          gene_operator = gene_clusters_operator.value\n",
    "          if gene_clusters_int.value <= 0:\n",
    "            gene_number = 1\n",
    "          else:\n",
    "            gene_number = gene_clusters_int.value\n",
    "\n",
    "        print(f\"\\nCutoff value: {cutoff_value}\")\n",
    "        print(f\"Minimum number of pathways: {num_pathways}\")\n",
    "        print(f\"Plot dendrogram: {plot_dendrogram}\")\n",
    "        print(f\"Print gene cluster memberships: {print_gene_cluster_membership}\")\n",
    "        print(f\"Download cluster file: {download_file}\\n\")\n",
    "\n",
    "    # Function to find the compactness/relatedness of pathways in a cluster\n",
    "    def compute_intra_cluster_compactness(clusters, similarity_matrix):\n",
    "        compactness_scores = {}\n",
    "\n",
    "        for cluster_label, pathway_indices in clusters.items():\n",
    "            # Generate all pairs of pathways within the cluster\n",
    "            pairs = list(combinations(pathway_indices, 2))\n",
    "\n",
    "            if not pairs:  # Single pathway in the cluster\n",
    "                compactness_scores[cluster_label] = 1.0  # Max compactness for single element\n",
    "                continue\n",
    "\n",
    "            # Calculate average similarity for all pairs\n",
    "            total_similarity = sum(similarity_matrix[i, j] for i, j in pairs)\n",
    "            compactness_scores[cluster_label] = total_similarity / len(pairs)\n",
    "\n",
    "        # Calculate overall average compactness across all clusters\n",
    "        if len(compactness_scores) == 0:\n",
    "          total_compactness = 0\n",
    "        else:\n",
    "          total_compactness = round(sum(compactness_scores.values()) / len(compactness_scores), 3)\n",
    "\n",
    "        print(\"Intra-cluster compactness calculated\")\n",
    "        return compactness_scores, total_compactness\n",
    "\n",
    "    # Function to compute similarity between two gene lists\n",
    "    def compute_similarity(set1, set2):\n",
    "        intersection = set1.intersection(set2)  # Number of genes matching between the pathways\n",
    "        union = set1.union(set2)  # Total number of genes in both the pathways (without duplicates)\n",
    "        return len(intersection) / len(union) if union else 0\n",
    "\n",
    "    # Function to create a similarity matrix (percentage of genes matching between two pathways) from the CSV file\n",
    "    def create_similarity_matrix(input_file):\n",
    "        total_genes = set()\n",
    "\n",
    "        with open(input_file, \"r\") as file:\n",
    "            reader = list(csv.reader(file))\n",
    "            headers = reader[0]  # First row is the header\n",
    "            data = reader[1:]  # Exclude the header\n",
    "\n",
    "            # Extract pathway names and gene sets\n",
    "            pathway_names = [row[0] for row in data]  # Extracts the pathway names from the first column\n",
    "            gene_sets = [set(gene.strip() for gene in row[3].split(\",\")) for row in data]  # Extracts the genes from the 4th column and comma separates them, .strip() makes it space insensitive\n",
    "\n",
    "            # Initialize similarity matrix\n",
    "            n = len(data)\n",
    "            similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "            # Compute pairwise similarity\n",
    "            for i in range(n):\n",
    "                total_genes = total_genes.union(gene_sets[i]) # Extracts all the genes in all pathways (without duplicates)\n",
    "                for j in range(n):\n",
    "                    similarity_matrix[i, j] = compute_similarity(gene_sets[i], gene_sets[j])\n",
    "\n",
    "            print(\"Similarity matrix created\")\n",
    "            return similarity_matrix, pathway_names, total_genes, gene_sets\n",
    "\n",
    "    # Function to perform hierarchical clustering and visualize the dendrogram\n",
    "    def hierarchical_clustering(similarity_matrix, pathway_names, total_genes, gene_sets, plot_dendrogram, cutoff_value, num_pathways, print_gene_cluster_membership):\n",
    "        single_cluster = set()\n",
    "\n",
    "        # Convert similarity matrix to a distance matrix (percentage of genes that differ)\n",
    "        distance_matrix = squareform(1 - similarity_matrix)\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        linkage_matrix = linkage(distance_matrix, method=\"average\")\n",
    "\n",
    "        # Plot the dendrogram if requested\n",
    "        if plot_dendrogram == True:\n",
    "          plt.figure(figsize=(12, 8))\n",
    "          dendrogram(linkage_matrix, labels=pathway_names, leaf_rotation=90, leaf_font_size=8)\n",
    "          plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "          plt.xlabel(\"Pathway\")\n",
    "          plt.ylabel(\"Cutoff Value (% Different Genes)\")\n",
    "          y_min, y_max = plt.ylim()\n",
    "          plt.yticks(np.arange(y_min, y_max, 0.1))  # Set ticks from min to max in 0.1 steps\n",
    "          plt.show()\n",
    "\n",
    "        # Form clusters\n",
    "        cutoff_value = cutoff_value # Maximum allowed difference between pathways in a cluster\n",
    "        num_pathways = num_pathways  # Minimum allowed number of pathways in a cluster\n",
    "        cluster_labels = fcluster(linkage_matrix, cutoff_value, criterion=\"distance\") # Forms flat clusters (an array) from hierarchical clustering\n",
    "\n",
    "        # Organize pathways into clusters\n",
    "        clusters_names = {}\n",
    "        for pathway, label in zip(pathway_names, cluster_labels):\n",
    "            if label not in clusters_names:\n",
    "                clusters_names[label] = []  # Initialize an empty list for this cluster\n",
    "            clusters_names[label].append(pathway)  # Add the pathway to its corresponding cluster\n",
    "\n",
    "        filtered_clusters_names = {key: value for key, value in clusters_names.items() if len(value) >= num_pathways} # Keep only clusters that have at least num_pathways pathways\n",
    "\n",
    "        # Organize pathways into clusters using indices instead of names\n",
    "        clusters_index = {}\n",
    "        for idx, label in enumerate(cluster_labels):\n",
    "            clusters_index.setdefault(label, []).append(idx)  # Store pathway indices instead of names\n",
    "\n",
    "        filtered_clusters_index = {key: value for key, value in clusters_index.items() if len(value) >= num_pathways} # Keep only clusters that have at least num_pathways pathways\n",
    "\n",
    "        # Compute compactness\n",
    "        compactness_scores, total_compactness = compute_intra_cluster_compactness(filtered_clusters_index, similarity_matrix)\n",
    "\n",
    "        # Initalize arrays\n",
    "        pathway_cluster = []\n",
    "        pathway_id = []\n",
    "\n",
    "        gene_cluster_counts = {}  # Dictionary to store how many different clusters each gene appears in\n",
    "        gene_cluster_ids = {} # Dictionary to store which clusters each gene belongs to\n",
    "\n",
    "        # Iterate over each cluster and its corresponding pathway indices\n",
    "        for cluster_id, indices in filtered_clusters_index.items():\n",
    "            unique_genes_in_cluster = set()\n",
    "\n",
    "            # Collect all genes from the pathways in the current cluster\n",
    "            for idx in indices:\n",
    "                unique_genes_in_cluster.update(gene_sets[idx])\n",
    "            for gene in unique_genes_in_cluster:\n",
    "                # Increment the count of clusters the gene appears in\n",
    "                gene_cluster_counts[gene] = gene_cluster_counts.get(gene, 0) + 1\n",
    "\n",
    "                # Add the cluster ID to the set of clusters the gene is found in\n",
    "                gene_cluster_ids.setdefault(gene, set()).add(cluster_id)\n",
    "\n",
    "        # Write clusters to a CSV file\n",
    "        with open(filtered_P_and_Q_file, \"r\") as in_file, open(specific_path_cut_file, \"w\", newline=\"\") as out_file:\n",
    "            reader = list(csv.reader(in_file))\n",
    "            writer = csv.writer(out_file)\n",
    "            writer.writerow([\"Cluster ID\", \"Pathways\", \"Compactness Score\", \"Cluster Label\"])\n",
    "\n",
    "            pathways_expressed = 0\n",
    "            data = reader[1:]\n",
    "            unique_genes = set()\n",
    "\n",
    "            for cluster_id, pathway_list in filtered_clusters_names.items():\n",
    "              compactness_score = compactness_scores.get(cluster_id, 0)  # Get the compactness score for this cluster\n",
    "              writer.writerow([cluster_id, \", \".join(pathway_list), f\"{compactness_score:.3f}\", \", \".join(word_frequency(pathway_list))])  # Write the cluster ID, pathways in the cluster, and average compactness score to the file\n",
    "              pathways_expressed += len(pathway_list)  # Tracks the number of pathways written to the output file\n",
    "\n",
    "              for pathway in pathway_list:\n",
    "                unique_genes = unique_genes.union(gene_sets[[row[0] for row in data].index(pathway)]) # Tracks the genes that belong to the pathways written to the output file (without duplicates)\n",
    "                pathway_cluster.append((pathway, cluster_id))\n",
    "\n",
    "        # Print statements\n",
    "        print(\"\\nPathways expressed in file:\", pathways_expressed)\n",
    "        print(\"Total pathways:\", len(data))\n",
    "        print(\"Percentage of pathways expressed:\", f\"{(pathways_expressed/len(data))*100:.2f}%\")\n",
    "\n",
    "        print(\"\\nGenes expressed in file:\", len(unique_genes))\n",
    "        print(\"Total genes:\", len(total_genes))\n",
    "        print(\"Percentage of genes expressed:\", f\"{(len(unique_genes)/len(total_genes))*100:.2f}%\")\n",
    "\n",
    "        print(\"\\nNumber of clusters:\", len(filtered_clusters_names))\n",
    "        print(\"Average compactness score of file:\", total_compactness)\n",
    "\n",
    "        # Print gene cluster membership if requested\n",
    "        if print_gene_cluster_membership == True:\n",
    "          print(\"\\nThe clusters each gene belongs to:\")\n",
    "          if gene_operator == \"less than or equal to\":\n",
    "            # Iterate through genes\n",
    "            for gene, count in gene_cluster_counts.items():\n",
    "              if count <= gene_number:\n",
    "                single_cluster.add(gene)\n",
    "\n",
    "                # Print the gene name and the clusters it is found in\n",
    "                cluster_list = \", \".join(map(str, sorted(gene_cluster_ids[gene])))\n",
    "                print(f\"{gene}: {count} clusters (Cluster IDs: {cluster_list})\")\n",
    "          if gene_operator == \"exactly\":\n",
    "            # Iterate through genes\n",
    "            for gene, count in gene_cluster_counts.items():\n",
    "              if count == gene_number:\n",
    "                single_cluster.add(gene)\n",
    "\n",
    "                # Print the gene name and the clusters it is found in\n",
    "                cluster_list = \", \".join(map(str, sorted(gene_cluster_ids[gene])))\n",
    "                print(f\"{gene}: {count} clusters (Cluster IDs: {cluster_list})\")\n",
    "          if gene_operator == \"greater than or equal to\":\n",
    "            # Iterate through genes\n",
    "            for gene, count in gene_cluster_counts.items():\n",
    "              if count >= gene_number:\n",
    "                single_cluster.add(gene)\n",
    "\n",
    "                # Print the gene name and the clusters it is found in\n",
    "                cluster_list = \", \".join(map(str, sorted(gene_cluster_ids[gene])))\n",
    "                print(f\"{gene}: {count} clusters (Cluster IDs: {cluster_list})\")\n",
    "\n",
    "    # Function to print the word frequency of each cluster for one specific file\n",
    "    def word_frequency(pathway_list):\n",
    "        # Common filler words or other unhelpful naming words\n",
    "        common_words = {\"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"it\", \"for\", \"on\", \"with\", \"as\", \"was\", \"at\", \"by\", \"an\", \"homo\", \"sapiens\", \"human\", \"humans\",\n",
    "                        \"saccharomyces\", \"cerevisiae\", \"budding\", \"yeast\", \"mus\", \"musculus\", \"mouse\", \"mice\", \"between\"}\n",
    "\n",
    "        cluster = \", \".join(pathway for pathway in pathway_list)\n",
    "\n",
    "        words = re.findall(r'\\b\\w+(?:-\\w+)*\\b', cluster.lower())  # Extract words and convert to lowercase (treats hyphenated words as one word)\n",
    "        words = [word for word in words if word not in common_words]\n",
    "        word_counts = Counter(words)  # Count word frequencies\n",
    "\n",
    "        if word_counts:\n",
    "            max_frequency = max(word_counts.values())  # Find the highest word frequency value\n",
    "            most_frequent_words = [word for word, count in word_counts.items() if count == max_frequency]  # Store the word(s) with this frequency value\n",
    "\n",
    "        sorted_rows = sorted(most_frequent_words, key=lambda x: x[0])  # Sort alphabetically\n",
    "\n",
    "        return sorted_rows\n",
    "\n",
    "    # File paths\n",
    "    specific_path_cut_file = f\"{source_organism}_{cofactor_name}_{num_pathways}_{cutoff_value}.csv\"\n",
    "\n",
    "    # Function calls\n",
    "    # Step 1: Create the similarity matrix\n",
    "    similarity_matrix, pathway_names, total_genes, gene_sets = create_similarity_matrix(filtered_P_and_Q_file)\n",
    "\n",
    "    # Step 2: Perform hierarchical clustering\n",
    "    hierarchical_clustering(similarity_matrix, pathway_names, total_genes, gene_sets, plot_dendrogram, cutoff_value, num_pathways, print_gene_cluster_membership)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Download the output file if requested\n",
    "    if download_file == True:\n",
    "      files.download(specific_path_cut_file)\n",
    "\n",
    "# Attach observer to dropdown\n",
    "gene_clusters_TF.observe(toggle_extra_inputs, names='value')\n",
    "\n",
    "# Attach event listener\n",
    "submit_button.on_click(on_button_click)\n",
    "\n",
    "# Display all widgets\n",
    "display(row1, row2, row3, row4, extra_inputs_box, row5, submit_button, output)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
